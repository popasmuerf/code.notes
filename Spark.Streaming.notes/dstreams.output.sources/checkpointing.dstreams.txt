DStream Basics
=============================================================================================


What is a DStream ?
-------------------------------------------------
	* Discretized Stream (DStream)
	* basic absraction in Spark Streaming
	* continuous sequence of RDDs(of the same type)
	  representing a cotinous stream of data
	* DStreams can either be created from live data or
	  it can be generated by transformation existing DStreams using
	  transformationss on existing DStreams



DStream operations
===========================================================================================

Transformations
-------------------------------------------------------
Transformation on DStreams can be grouped into either:

	*	Stateless transformations
		-	In stateless transformations the processing of
			each bach does not depened on the data of its 
			previous batches. this includes:
				*	Common RDD transformations:
						-	filter
						-	map
						-	reduce
						-	reduceByKey
						-	flatMap()
						-	groupByKey()

				*	All of the above will only perform
					transformations on each time shard
					or mini batch...but not accross shards
					or batches.

				*	There is a way to combine stateless transformation data
					across time

	*	Statefule transformations
		-	Uses data or intermediate results
			fromprevious batches to compute the results
			from previous batches to compute the results 
			of the currentbatch based on:
				*	Sliding windows
				*	Tracking state accross time



Code examples for Stateless transformations
============================================================================

/**In scala...assuming we are using ApacheAccessLog lib/api **/

val accessLogDstream = logData.map(line => ApacheAccessLog.parseFromLogLine(line))
val ipDStream = accessLogsDStream.map(entry => (entry.getIpAdress(),1))
val ipCountDstream = ipDStream.reduceByKey((x,y) => x + y)


/**In Java...assuming we are using ApacheAccessLog lib/api **/

Static final calss implements PairFunction<ApacheAccessLog,String, Long>{
	public Tuple2<String, Long> call(ApacheAccessLog log){
		return Tuple2<>(log.getIpAddress(), 1L)
	}
}

JavaDstream<ApacheAccessLog> accessLogsDStream = logData.map(new ParseFromLine());
JavaPairDStream<String,Long> ipDSream = acessLogsDStream .ampToPair(new IpTuple());
JavaPairDStream<String, Long> ipCountsDStream = ipDStream.reduceByKey(new LongSumReducer());



Joining  Stateless DStreams
=============================================================

We do it just like we would with RDDs:

//-------code
val dStreamC = dStreamA.join(dStreamB)

//----or
val dStreamC = dStreamA.union(dStreamB)



Transforming RDDs within a DStream directly
=================================================
If stateless transformatons are insufficienct, DStreams
provides for more advanced operations via the method:

	transform()

which allows for arbitrary RDD-to-RDD functions

The function is called on each batch of data in the stram to
produce a new stream.   You can use the transform method to make
use of exiting code targeting RDDs.

//-----------code:
val outlierDStream = accessLogsDStream.transform {rdd => parseLogs(rdd)}



Apache Kafka
==========================================================
Maven artifact 
---------------------------------------------------
spark-streaming-kafka_2.1x



How does it work ?
==============================================
We need to modify our application into
a Kafka consumer.  There is an object :

	KafkaUtils

that works on:

	StreamingContext
		-and-
	JavaStreamingContext

to create a DStream of your kafka messages.


Example of Apache Kafka subscribing to Panda's topic in
Scala
====================================================================
imort org.apache.spark.streaming.kafka._
//Create a map of toics to unumbr of reciever threads to use
val topics = List(("topicA",1)("topicB",1),("topicC",1)).toMap
val topicLines = KafkaUtils.createStream(ssc,zkQourum, group, topics)
StreamingLogInput.processLines(topicLines.map(_._2) 




Checkpointing 
====================================================================
Checkpointing is the main mechanism that need to be set up for fault 
tolerance in Spark Streaming.  It allows Spark Streaming to periodicaly
save data about the applicaton to a reliable storage system, such
as HDFS or Amazon S3, fo rue in recovring.


Purposes served by Checkpointing
=======================================================
	*	Limiting the state that must be recomputed on failure.
	*	Spark Streaming can recompute state
	using the lineage graph of transformations, but 
	checkpointing controls how far back it must go
	*	Fault tolerance for the driver program sould the application
		crashes...we can launch it again and tell it to
		recover from a check point, in which case Spark Streaming
		will read how far the prvious run of the program got
		w/r to processing the data



Setting up a Checkpoint
=============================================================
val checkpointDir = "/var/log/checkpointdir"
......
val sc = new SparkContext(conf)
val ssc = new StreamingContext(sc, Seconds(1))
ssc.checkpoint(checkpointDir)