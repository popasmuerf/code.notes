spark.jobserver.SparkJob

Nan
John Uchiama

https://github.com/spark-jobserver/spark-jobserver/blob/master/README.md#getting-started-with-spark-job-server

How do we create a job that can be submitted
through a job server.... 
========================================
Requirements
--------------
*	job must implement the
	trait:
		SparkJob

Constraints
---------------
*	None

//---------Example Code----------//

object SampleJob extends SparkJob{
	override def runJob(sc:SparkContext,jobConfig:Config):Any= ???
	orveride def validate(sc:SparkContext,config:Config):SparkValidate = ??
}


^^^What's going on here ?

*	runJob:

	-Contains the implementation of the Job
	-The SparkContext is managed by the
	 JobServer and will be provided
	 to the job through this method.
	-Programmers are relieved from the
	 work of having to create a 
	 Spark job and allows the Job Server
	 to manage contexts

*	validate:

	-Allows for an initial validateiong
	 of the context and any provided 
	 configuration.
	-If the configuration is OK
	 then run the job returning:

	 	spark.jobserver.SparkJobValid

	-If the configuration is not valid then
	 the following is returned:

	 	spark.jobserver.SparkJobInvalid(reason)

	 which prevnts the job from running and provides means to convey the reason of 
	 failure. In this case the call 
	 will immediately return an:

	 	HTTP/1.1 400 Bad Request

	-validate helps you preventing running
	 jobs that will eventually fail
	 due to missing or wrong configuration


Some more example code for creating
a Job for Spark JobServer
======================================
Important note:
Note: As of version 0.7.0, a new SparkJob API that is significantly better than the old SparkJob API will take over. Existing jobs should continue to compile against the old spark.jobserver.SparkJob API, but this will be deprecated in the future. Note that jobs before 0.7.0 will need to be recompiled, older jobs may not work with the current SJS example. The new API looks like this:
-----------------------------------------
//-----------code------------//

object WordCountExampleNewApi extends NewSparkJob{
	
	type JobData = Seq[String]
	type JobOutput = collection.Map[String,Long]

	def runJob(sc:SparkContext, runtime:JobEnviroment, data: JobData):JobOutput={
		sc.parallelize(data).countByValue
	}

	def validate(sc:SparkContext, runtime:JobEnviroment,config:Config):JobData Or Every[ValidationProblem]={
		Try(config.get("input.string").split(" ").toSeq).map(words => Good(words))
		.getOrElse(Bad(One(SingleProblem("No input.string param"))))
	}

}


^^^^Running the above code
-------------------------------------
curl -i -d "bad.input=abc" "localhost:8090/jobs?appName=test&classPath=spark.jobserver.WordCountExample"

HTTP/1.1 400 Bad Request
Server: spray-can/1.2.0
Date: Tue, 10 Jun 2014 22:07:18 GMT
Content-Type: application/json; charset=UTF-8
Content-Length: 929

{
  "status": "VALIDATION FAILED",
  "result": {
    "message": "No input.string config param",
    "errorClass": "java.lang.Throwable",
    "stack": ["spark.jobserver.JobManagerActor$$anonfun$spark$jobserver$JobManagerActor$$getJobFuture$4.apply(JobManagerActor.scala:212)",
    "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)",
    "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)",
    "akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:42)",
    "akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)",
    "scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)",
    "scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)",
    "scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)",
    "scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)"]
  }
}