## server class


A group of deployment clients.  Server classes facilitate the management 
of a set of deployment clients as a single unit.  


## inputs.conf and outputs.conf
    -   Location:
        *   $SPLUNK_HOME/etc/system/default/  #Do not edit this file
        *   $SPLUNK_HOME/etc/system/local/    #You can edit this file
        *   
    -   contains possible settings you can 
        use to configure inputs
            *   Distributed inputs
            *   Forwarders
            *   File system monitoring
        
    -   Each stanza controls different search commands settings
        *   

## What is a deployment sever ?
The deployment server is the tool for distributing:
    -   configurations
    -   apps
    -   content updates
to Splunk Enterprise intances.  
You can us it to distribute updates to most types of 
Splunk Enterprise components:

    -   Forwarders
    -   Non-clustered indexers
    -   search heads
    
    
### WTF is a "search head" 
In a distrbuted search enviroment, a Slunk Enterprise
instance that handles search management function,
directin gsearch requests to a set of search peers
and then merging th eresults back to the user



A Splunk Enterprise instance can function as both a search head and 
a search peer. A search head that performs only searching, and
not any indexing, is referred to as a dedicated search head.a



The 



The deployment server is the tool for dis


A server class can group deployment clients by:
    -   Application
    -   Operating system
    -   Data type 
    
[](About deployment server and forwarder management)
   
   
   
## About deployment server and forwarder management

   
    
to be indexed or any other feture of a Splunk
Enterprise deployment.


A deployment server uses server classes to determine whawt content to deoploy
to groups of deployment clients  The forwarder managerment
interface offers an easy way to:

    -   create 
    -   endit
    -   manage 
   
Sever classes.





## Installation Options
Universal Forwarder collects data from a server and send it to your Splunk 
Deploymnet.


Splunk offers univeral forwarder agent for various operating systems, including:

-   Linux
-   Windows
-   MAC
-   Solaris 
-   FreeBSD
-   AIX





## Overview of Workflow Actions


Splunk WorkFlow Actions allows us to add interactivity between the indexed fields and 
other web resources

## Example:

There is a field called as clientip in access_combined log file.
We can add option for "Whois Lookup" ased on the IP address in 
clientip field.

### Splunk transforms incoming data into events, which it stores in the indexes...


1. Data goes into Splunk
2. Splunk transforms this data into events
3. Splunks stores events into indices


### What Splunk does with this data revisted...

-   When Splunk indexes your data, it creates a number of files.
-   These files falls into two main categories
    *   The raw data in compressed from (raw data)
    *   Indexes that points to raw data (tsdix files), plus some meta-data files
    
These files resides in set of directories organized by age


    
    
    
## Default Set of Indexes

Splunk Enterprise comes with a number of pre-configured indexes, including:

| main      | This is the default index.   All data gets stored here unless specified.                         |
|-----------|--------------------------------------------------------------------------------------------------|
| _internal | Stores Splunk's internal logs                                                                    |
| _audit    | Contains events related to user search history, file system change monitor and auditing specific |
|           |                                                                                                  |






Splunk stores all it's data in directories on the server called "buckets".
A bucket moves through sveral stages as it ages:
    -hot
    -warm
    -cold
    -frozen
    
    
    
| main      | This is the default index.   All data gets stored here unless specified.                         |
|-----------|--------------------------------------------------------------------------------------------------|
| _internal | Stores Splunk's internal logs                                                                    |
| _audit    | Contains events related to user search history, file system change monitor and auditing specific |
|           |                                                                                                  |
    


New/Recent data --> Hot Bucket #Read/Write
Hot Bucket --> Warm Bucket  #Read only
Warm Bucket --> Cold Bucket #Read only, reduced chance of being read, Data is usually archived
Cold Bucket --> Frozen, data is deleted by Splunk by default, setting can be changed for archival
Thawed --> If data in frozen bucket is archived, it can be indexed again by thawing it




## Hot Bucket to Warm Bucket

Buckets are rolled from hot to warm in the following condition:

*   Too man hot buckets [maxHotBuckets]
*   Hot bucket has not recieved data since a while
*   Timespan of buckets is too large
*   Index clustering repliction error
*   Splunk is restarted





## Warm to Cold Buckets

-  Ideally, historical data should go here
-  Allows us to keep older data on slower(cheaper storage)
-  Buckets are rolled from warm to cold when there are too many warm buckets.


[index_name]
coldPath = $SPLUNK_DB/$_index_name/colddb
maxWarmDBCount = 300



### What do we mean by too many warm buckets ?



## Cold to Fron Buckets

-  Data in fronzen is no longer searchable
-  Data rolls from cold to frozen bucket when:
    *   Total Size of index (hot + warm + cold) grows to large
    *   Oldest event in bucket exceeds specific age
    *   Default process tsidx file is removed and bucket is specified to
        esitination we specify.



## Thawing Process 

Overall Steps:
i) mv/tmp/frozendb/db*$SPLUNK_HOME/var/lib/splunk/defaultdb/thawddb/

ii) splunk rebuild $SPLUNK_HOME/var/lib/splunk/defaultdb/thaweddb/db*

iii) splunk restart





































