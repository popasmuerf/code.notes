Scrapy tutorial
============================================
https://doc.scrapy.org/en/latest/intro/tutorial.html

Generate scrapy project
======================================
> scrapy startproject <project_name>

Generated project directory structure
---------------------------------------
tutorial/
    scrapy.cfg            # deploy configuration file

    tutorial/             # project's Python module, you'll import your code from here
        __init__.py

        items.py          # project items definition file

        middlewares.py    # project middlewares file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py




Our first Spider
===================================================
What are spiders(at least as far as Scrapy is conscerned
) are :

	*	classes that you define that Scrapy uses to scrape
		information frma website or groups of websites

	*	all classes must subclass:
			-	scrapy.spider

	*	classes must define the inital rquests to make

	*	classes must define how to follow links in the 
		pages if required

	*	classes must define how to parse the downloaded page
		content to extract data




Example : Code for first Spider 
==========================================================
import scrapy

class QoutesSpider(scrapy.Spider):
	name = "qoutes"
	#Define initial request
	def start_requests(self):
		 urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
       	for url in urls:
       		yield scrapy.Request(url=url, callback=self.parse)

    #Define how you will parse downloaded pages
    #or rather the response from the requests
    def parse(self, response):
    	page = response.url.split("/")[-2]
    	filename = filename = 'quotes-%s.html' % page
    	with open(filename,'wb') as f:
    		f.write(response.body)
    	self.log('Saved file %s' % filneame)


^^^What's going on here ?

*	name:
		Identifies the Spider.  It must be unique within a project,that is,
		You cannot set the same name for different spiders

*	start_requests():
		must reutn an iterable of Requests(you can return a list of rquests
		or write a generator function) which the spider will
		begin to crawl from.  Subsequent rquests will
		be generated successively from these initial requests

*	parse():
		A method that will be called to handle the response downoaded
		for each of the requests made.  The response parameter
		is an instance of:
			TextResponse
		that holds the page content and has further helpful
		methods to handle it.

		The parse() method ususally pares the response,
		extracting the scraped data as dictionares and also
		finding new URLs to follow and creating 
		new requests(Request) from tem.


Running our Spider(I have scrapy installed in a python
virtualenv )
=========================================================
>(scrapy) Michaels-MacBook-Pro:tutorial mdb$ scrapy crawl qoutes
2018-01-26 12:48:31 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: tutorial)
2018-01-26 12:48:31 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.19.0, Twisted 17.9.0, Python 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:05:08) - [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Darwin-16.7.0-x86_64-i386-64bit
2018-01-26 12:48:31 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tutorial'}
2018-01-26 12:48:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-01-26 12:48:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-01-26 12:48:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-01-26 12:48:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-01-26 12:48:31 [scrapy.core.engine] INFO: Spider opened
2018-01-26 12:48:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-01-26 12:48:31 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-01-26 12:48:31 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)
2018-01-26 12:48:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
2018-01-26 12:48:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://quotes.toscrape.com/page/1/> (referer: None)
Traceback (most recent call last):
  File "/Users/mdb/python_env/scrapy/lib/python2.7/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/mdb/python_env/scrapy/tutorial/tutorial/spiders/QoutesSpider.py", line 14, in parse
    filename = 'quoutes-%.html' % page
ValueError: unsupported format character 't' (0x74) at index 11
2018-01-26 12:48:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)
2018-01-26 12:48:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://quotes.toscrape.com/page/2/> (referer: None)
Traceback (most recent call last):
  File "/Users/mdb/python_env/scrapy/lib/python2.7/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/mdb/python_env/scrapy/tutorial/tutorial/spiders/QoutesSpider.py", line 14, in parse
    filename = 'quoutes-%.html' % page
ValueError: unsupported format character 't' (0x74) at index 11
2018-01-26 12:48:32 [scrapy.core.engine] INFO: Closing spider (finished)
2018-01-26 12:48:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 678,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 5976,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 1, 26, 17, 48, 32, 37647),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 46137344,
 'memusage/startup': 46137344,
 'response_received_count': 3,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2018, 1, 26, 17, 48, 31, 241407)}
2018-01-26 12:48:32 [scrapy.core.engine] INFO: Spider closed (finished)
(scrapy) Michaels-MacBook-Pro:tutorial mdb$ 


^^^What is going on here ?
===========================================
We just an the spider named "qoutes"...
the spider then did this:

	*	Sent requests to qoutes.toscrape.com
		and then send req:
			def start_requests(self):
				 urls = [
		            'http://quotes.toscrape.com/page/1/',
		            'http://quotes.toscrape.com/page/2/',
		         ]
		       	for url in urls:
		       		yield scrapy.Request(url=url, callback=self.parse)


Shortcuts to the start_requests method
==================================================
Instead of implementing the method "start_requsts()"
method that generates instances of class scrapy.Request
objects from URLS, you can just define a 
start_urls class attribute with a list of URLS.  Ths list wil then
be used by the default implementatio of start_requests()
to create the inital requests for your spider:

'''
code Example...
'''

import scrapy

class QuotesSpider(scrapy.Spider):
	name = "qoutes"
	  start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
    	page = response.url.split("/")[-2]
    	filename = 'quotes-%s.html' % page
    	with open(filename,'wb') as f:
    		f.write(response.body)

    





