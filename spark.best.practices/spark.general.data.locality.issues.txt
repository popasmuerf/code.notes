https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html

Data Locality
=============================================
Spark is a data parallel processing framework, which means it will execute tasks
as close to where the data lives as possible



*	http://localhost:4040/stages/


Checking Locality
==========================================================================
The best means of checking whether a task ran locally is to inspect a given stage in the 
Spark UI ....you can see this awesomeness in the staging URL:


*	http://localhost:4040/stages/


Adjusting Locality Configurations
=============================================
You can adjust how long Spark will wait before it times out on each of the phases of data locality (data local --> process local --> node local --> rack local --> Any). For more information on these parameters, see the spark.locality.* configs in the Scheduling section of the Application Configration docs.






